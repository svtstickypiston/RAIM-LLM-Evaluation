# -*- coding: utf-8 -*-
"""llmsurveyevaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tWzjL5SfKoyOj2h8eIiQK4YgXdcZmh98
"""

# -*- coding: utf-8 -*-
"""raim_llm_evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BHSn5DLtPhOkqWM_fPTNVah2FM0XweQn

# Responsible AI Music (RAIM) Survey via LLM Personas

To systematically explore the perceived importance of different responsible AI features within the music domain across diverse stakeholder groups, this study will employ Large Language Models (LLMs) to simulate participant personas and generate corresponding survey responses. This methodology, grounded in recent research demonstrating the potential of LLMs for survey response generation and persona simulation (e.g., Argyle et al., 2023; Santu & Feng, 2024; Wang et al., 2024), offers a valuable approach for gathering preliminary insights efficiently. Leveraging the carefully designed survey instrument, which has already undergone iterative refinement through human pilot testing, we will programmatically generate 10 distinct personas for each of the four target user groups (AI Experts, Ethics/Law Experts, Creative Artists, Music Listeners), resulting in a dataset of 40 simulated responses.

Each persona will be defined by a short profile generated by an LLM based on the study's background information and inclusion criteria. Subsequently, an LLM instance, instructed to adopt the specific persona, will complete the survey by rating the importance of features presented sequentially by pillar, mirroring the intended flow of the Microsoft Forms survey. This approach not only facilitates the rapid collection of a structured dataset (CSV format) for initial analysis but also mitigates potential confounds associated with human participant engagement, such as survey fatigue or declining attention over the course of rating multiple features.

While acknowledging the simulated nature of the data, this method provides a controlled environment to systematically examine how different stakeholder perspectives might value various facets of responsible AI in music, offering valuable preliminary patterns and hypotheses that can complement or guide future research involving human participants.

Link to the survey: https://forms.office.com/Pages/ResponsePage.aspx?id=MVElUymxEECG4UdL_X6AdgmACyNNFlVJmNi9LQSOMHRUN0IyMTNPSURFTEFRWE9HRDM4Qk1TU09IUi4u

Prompting via Google Gemini API (https://ai.google.dev/gemini-api/docs/text-generation)
"""

from google import genai
from pydantic import BaseModel

import pandas as pd
import numpy as np
import json
import os

GEMINI_API_KEY = "Add your API key here"

"""## Survey structure

### Common sections
"""

INFORMATION_SHEET = """
Responsible AI in Music
To inform the next generation of Responsible Music AI systems, we are collecting insights into which features such generative systems should have in order to be accepted, trusted, and ethically approved by creative professionals . In this survey, you will be asked to provide general information about your background (no personal data is collected); and rate the relevance of each responsible features we have identified so far. You should also refer to the descriptions of the features as well as the wider study, available on our website: https://amresearchlab.github.io/raim-framework/
The survey should take approximately 5-15 minutes. Thank you for your time and for contributing to our research.

Project Title: Understanding Feature Importance for Responsible AI Music
Purpose of the Study: This study aims to understand how important different features are for designing Responsible Generative Artificial Intelligence (AI) in the music domain. Your input will help us develop guidelines for building generative music tools that are ethical, fair, and beneficial.
Procedure: You will be asked to rate a set of features related to Responsible AI music systems. We will ask you to rate the importance of each feature using the following scale:
1: Not at all Important: (This feature is completely unnecessary for responsible music AI.)
2: Slightly Important: (The feature has minor relevance but is not a key factor.)
3: Neutral: (The feature is neither important nor unimportant; its importance is unclear or depends on the situation.)
4: Important: (The feature is a valuable and significant part of responsible music AI.)
5: Essential: (The feature is critical for responsible music AI; it is a must-have.)
The survey will take approximately 15-20 minutes to complete.

Data Use and Anonymity: Your responses will be completely anonymous. We will not collect any personally identifiable information (like your name, email address, or IP address). The data will be used for research purposes only, such as academic publications and presentations. Results will be reported in aggregate form (summarised data, not individual responses).
Voluntary Participation and Withdrawal: Your participation is entirely voluntary. You can choose not to participate or to withdraw from the study at any time without any penalty or consequence. You may skip any questions you do not wish to answer.
Minimal Risks: This study involves minimal risk. This means the risks associated with participation are no greater than those encountered in everyday life.
Benefits: There are no direct personal benefits to you for participating. However, your input will contribute to the development of more responsible and ethical AI systems in the music domain.
"""

BACKGROUND_SECTION = """
Responsible AI in Music

- Background Information. Please provide some background information about yourself. No personal data will be collected.
Which continent are you based in?
[ ] Europe
[ ] Asia
[ ] North America
[ ] South America
[ ] Africa
[ ] Oceania

- How do you rate your current level of musical training? Please, only chose the most representative option.
[ ] None
[ ] I have received informal musical training or am self-taught
[ ] I have received formal musical training
[ ] I have received postgraduate musical training at an institute
[ ] I have received professional training with a musical company

- What is your experience with a Generative Music AI system?
[ ] I have never used a Music AI system
[ ] I have used a Music AI system only once
[ ] I use Music AI system sometimes
[ ] I use Music AI systems on a regular basis

- Have you ever contributed to the design, implementation, or evaluation of a Music AI system? If so, please tick all the options that apply.
[ ] Design
[ ] Implementation
[ ] Evaluation

- Define your current occupation, by choosing the option that fits your work best. Please, note that each option should be ticked only if related to professional practice, or your field of study in higher education.
[ ] Musician
[ ] Composer/Songwriter/Arranger
[ ] Producer
[ ] Audio Engineer
[ ] Ethicist
[ ] Lawyer
[ ] Legal Expert
[ ] AI Researcher
[ ] Computer Scientist
[ ] Music AI Expert
[ ] Other

- Group Selection Round 2. Because you said that you are involved in computer science, you will be asked about your competency in music and ethic/legal issues. Do you have more than 2 years of professional experience or 5 years of formal training for either legal/ethical matters or music?
[ ] Yes, for legal/ethical issues only
[ ] Yes, for music only
[ ] No for both
[ ] Yes for both
"""

FINAL_SECTION = """
Final Questions

- You have completed our survey! To conclude, we would like to know if you have any recommendations or comments on this questionnaire.
Please remember to record which group you were in so you can be part of future surveys!
Do you have any more features you would like to suggest, relating to one of the pillars you examined during the questionnaire? If so, please leave your suggestion below.
Please format your suggestion as such:
Pillar number: (leave a question mark if you are unsure which pillar would fit best)
Feature name:
Feature description:

- Do you have any general comments about the survey? If so, please list them below.
"""

"""### Pillar sections"""

PILLAR_1 = """
Pillar 1: Human Agency and Oversight
You may use our website (https://amresearchlab.github.io/raim-framework/#features-1) to ensure you fully understand the features in this pillar, as well as its context within the wider study.

Then, rate the importance of each feature for designing responsible music AI systems. Use the following scale:
1: Not Important At All: This feature is completely unnecessary for responsible music AI.
2: Slightly Important: This feature has some minor relevance, but is not a key factor.
3: Neutral: The feature is neither important nor unimportant; its importance is unclear or depends on the situation.
4: Important: This feature is a valuable and significant part of responsible music AI.
5: Essential: This feature is absolutely critical for responsible music AI. It is a must-have!

- Personalisation. The system can reuse the musical repertoire of the artist to personalise the style of the generation.
Personalisation allows a musician to create stylised music based on their existing repertoire, and the user will be able to influence the output of the system by personalising it based on certain songs.

- Creative Feedback. The system can iteratively refine or improve the generation based on the feedback provided by the artist throughout the creative process.
Creative feedback allows users to guide AI-generated music by providing input after each training cycle. This could improve the output quality and give users more control. Training would continue until the user is satisfied with the results.

- Controllability. To steer the generations depending on the artist’s preferences, the system provides a rich variety of modalities including language, melodic, harmonic, rhythmic, emotional control of the generation.
Controllability lets users shape AI-generated music through text prompts, melodies, harmonies, or rhythms. By inputting text or uploading files, users can actively influence the creation process, increasing their involvement and control over the final output.

- Red Button. If the system is generating and playing music that is either unpleasant, disturbing, or contains lyrics with offensive language, the user can always halt the generation process at any time.
To prevent distress from unsettling or unsafe music, users can use a "red button" to instantly stop the generation process and erase the produced music. This helps ensure immediate control and safety during creation.

- Safety Disclaimer. The system always advises the user whenever it can generate music that may be deemed dangerous or offensive to some categories of users (e.g. offensive language in lyrics).
A safety disclaimer would warn users of potential risks from harmful or offensive AI-generated content, helping to prevent harm and reduce legal issues. Upon starting, users would see an explanation and must confirm they understand the risks before proceeding.

- Deception Avoidance. The system does not show any deceptive behaviour related to the copyright and ownership of the generated material. For instance, the system never tries to claim ownership or novelty of the generations when music is clearly plagiarised.
To uphold copyright laws, the system will not claim ownership of music that may resemble training data or plagiarized content. It also avoids asserting novelty or ownership if the origins of its training data are uncertain.

- Artist Involvement. The system has been designed with the active involvement of creative professionals throughout its development cycle.
Artists' perspectives may be useful in AI music development, as they are most impacted by AI-generated music. Their insights can address ethical concerns and provide valuable input that computer scientists might overlook, ensuring the system meets ethical standards and practical needs.
"""

PILLAR_2 = """
Pillar 2: Robustness & Safety
You may use our website (https://amresearchlab.github.io/raim-framework/#features-2) to ensure you fully understand the features in this pillar, as well as its context within the wider study.

Then, rate the importance of each feature for designing responsible music AI systems. Use the following scale:
1: Not Important At All: This feature is completely unnecessary for responsible music AI.
2: Slightly Important: This feature has some minor relevance, but is not a key factor.
3: Neutral: The feature is neither important nor unimportant; its importance is unclear or depends on the situation.
4: Important: This feature is a valuable and significant part of responsible music AI.
5: Essential: This feature is absolutely critical for responsible music AI. It is a must-have!

- Music Leakage. The system does not not allow for the full or partial reconstruction of the music material used as training data unless this is explicitly acknowledged and allowed by the copyright holders of the music data.
To avoid copyright issues, it may be useful to ensure AI-generated music doesn’t reproduce training data without permission. This could involve tracking copyright information for all training data and comparing outputs to the training set. If similarity exceeds a threshold, the system would question copyright validity and discard the output if necessary.

- Generation fallback. If the system needs to generate music continuously over time, and the content is considered offensive at some point, the system can switch to a simpler generative strategy (e.g. a rule-based mode).
If the output is unsatisfactory or offensive, this feature allows the system to switch to a secondary generative strategy which uses pre-written rules to avoid unsafe content - though it may produce lower-quality music compared to the primary approach. This helps minimize negative impacts on users.

- Model Evaluation. The evaluation of the model/system is consistent with other frameworks and benchmarks for music generation.
Evaluating music generation is challenging due to its subjective nature. Assessment could be done using widely used methods from other models, combining objective metrics and subjective feedback (e.g., user ratings). This approach ensures a robust evaluation and comparison with similar systems.

- Music Evaluation. The system/model provides a comprehensive evaluation of the musical properties of its generations.
User may wish to understand the musical properties of AI-generated content, such as key, chords, and tempo. After generation, the system could provide a summary of these statistics, offering insights into the music’s structure and characteristics.

- Expert Evaluation. The evaluation of the system/model involves creative professionals or music experts.
Involving creative professionals in evaluation could ensure the system meets ethical standards and produces musically plausible outputs. A panel of music experts could provide feedback on both ethical considerations and the quality of the generated music.

- Model Availability. The computational model behind the generative system is fully publicly available.
Publicly releasing the model could improve transparency, allowing users to understand how the AI system works. This openness may help build trust, as people can see firsthand how the system generates content, reinforcing its credibility as a "trustworthy" AI.

- Training Data Availability. The training material on which the generative system relies is fully publicly accessible.
Releasing the training data for the system could increase the transparency of the system, as it might be useful for users to understand the source material that the software draws on.

- Prompt-to-Gen. If sample generations are released, the model can be seeded and prompted to recreate the same musical content.
Establishing a direct link between user prompts (and seeded training data) and system outputs may enhances transparency and reproducibility. It also shifts more creative control to the user, preserving the "human spirit" in the generated music.
"""

PILLAR_3 = """
Pillar 3: Privacy & Data Governance
You may use our website (https://amresearchlab.github.io/raim-framework/#features-3) to ensure you fully understand the features in this pillar, as well as its context within the wider study.

Then, rate the importance of each feature for designing responsible music AI systems. Use the following scale:
1: Not Important At All: This feature is completely unnecessary for responsible music AI.
2: Slightly Important: This feature has some minor relevance, but is not a key factor.
3: Neutral: The feature is neither important nor unimportant; its importance is unclear or depends on the situation.
4: Important: This feature is a valuable and significant part of responsible music AI.
5: Essential: This feature is absolutely critical for responsible music AI. It is a must-have!

- Prompt Leakage. The system does not distribute any personal data that was provided to prompt the model’s generations.
To prevent leaks of sensitive data, the system could delete user prompts shortly after use and screen both inputs and outputs for personal information. This minimizes the risk of sensitive data being exposed in the AI-generated content.

- Metadata Integrity. The system/model is trained on music data that is correctly attributed to the right authors.
To avoid copyright violations from misattributed training data, the system could store copyright information and warn users of potential issues before they use it. This prepares them for the possibility of copyright concerns related to the generated music.

- Safety of Training Data. The system is not trained on music data that can be deemed offensive or socially harmful (e.g. lyrics).
To prevent offensive outputs, the system could check training data for harmful content before generation. If detected, it would warn the user, ensuring they are aware of potential risks and can proceed with caution.

- Prompting Governance. If the system stores data collected from users through the generative process (e.g., prompts, feedback, music), access to it is explicitly regulated.
If user data is stored for future use (e.g., evaluations), access might be tightly regulated with clear protocols specifying who can access it and under what conditions. This is could be useful to address privacy concerns if the data includes personal information.

- Generative Reuse of Music Data. The model uses material (e.g. scores, audio recordings, lyrics) whose licensing and terms of use explicitly allow for training Generative systems.
To avoid conflicts, the system could use only music explicitly permitted for training - as music generation relies on human-created training data. While this may limit the volume of training material, it helps provide a more ethical and considerate approach toward creators.

- Copyright and Licensing of Generations. The system provides clear and comprehensive information about the copyright and licensing of the generations.
Users may need to understand the legal standing of AI-generated content to avoid misuse and potential legal issues. If the system holds copyright over generated music, this would be clearly communicated to the user to ensure proper usage and compliance.
"""

PILLAR_4 = """
Pillar 4: Transparency
You may use our website (https://amresearchlab.github.io/raim-framework/#features-4) to ensure you fully understand the features in this pillar, as well as its context within the wider study.

Then, rate the importance of each feature for designing responsible music AI systems. Use the following scale:
1: Not Important At All: This feature is completely unnecessary for responsible music AI.
2: Slightly Important: This feature has some minor relevance, but is not a key factor.
3: Neutral: The feature is neither important nor unimportant; its importance is unclear or depends on the situation.
4: Important: This feature is a valuable and significant part of responsible music AI.
5: Essential: This feature is absolutely critical for responsible music AI. It is a must-have!

- System Documentation. The design of the generative system is well-documented throughout all its development cycle. This includes all instructions needed to reproduce the implementation of the model (training) and generate data from it (sampling).
Proper documentation is may be useful for transparency, promoting honesty and providing a clear view of the project’s development. Documentation would follow established guidelines, such as including datasheets for each dataset to detail sources and other key characteristics.

- Evaluation Documentation. The evaluation of the model/system is well-documented and reproducible to make future evaluations (e.g. of other models) more consistent.
Proper documentation during evaluation is may be useful for transparency, ensuring honesty and demonstrating thorough, unbiased assessment. This includes detailing the model's performance, use cases, and potential biases, all of which could be clearly documented to maintain trust and accountability.

- Artefact Watermarking. The system automatically embeds a watermark into every generation to remark on their artificial nature.
Watermarking AI-generated music helps distinguish it from human compositions and prevents misuse, such as deepfakes or copyright violations. Tools like Meta’s AudioSeal can add imperceptible watermarks detectable only by specialized software, ensuring traceability and accountability for AI-created content.

- Generation Explainability. The system can explain how the generations are created in a way that is understandable to users.
Most users of music generation software, like musicians or music listeners, may not be AI experts. To improve transparency, the system could use concepts like harmonic memory, which generates chord sequences from user prompts. This approach is easier for musicians to understand and aligns with their existing knowledge.

- Data Explainability. The system can relate each generation to the training material that contributed to its creation process (e.g. a pattern, motive, or sample).
Matching training material to specific generations can help users understand how the AI constructs its outputs. This transparency builds confidence and makes the system more approachable, as users can easily trace and piece together the generation process.

- Artificial Awareness. Users interacting with the system are always aware of its artificial nature.
To ensure users understand they’re interacting with an AI system, not a human, they could be shown an information screen before use. This screen would explain how the music generation system works, and users might confirm they understand it before proceeding to help set clear expectations and avoid misconceptions.

- Benefits. The benefits of using the particular generative system, compared to other solutions, are communicated to users prior to its use.
To promote informed use, users could be shown an information screen before using the system, highlighting its technical and ethical benefits. This educates users on the system’s advantages, encouraging knowledgeable engagement with AI music generation tools.

- Limitations. The technical limitations and the potential risks of the generative system are communicated to users prior to its use.
To promote informed use, users could be shown an information screen before using the system, outlining its technical and ethical limitations. This educates users on the system’s constraints, encouraging realistic expectations when engaging with AI music generation tools.

- Instructional Material. Appropriate instructional material and disclaimers are provided to users on how to adequately use the system.
Providing instructional material, such as an information screen or tutorial, could help users, especially those less experienced with software, understand how to interact with the system effectively. This could include disclaimers, usage instructions, and a step-by-step guide to the controls, ensuring a smoother and more informed user experience.
"""

PILLAR_5 = """
Pillar 5: Diversity, Fairness and Non-discrimination
You may use our website (https://amresearchlab.github.io/raim-framework/#features-5) to ensure you fully understand the features in this pillar, as well as its context within the wider study.

Then, rate the importance of each feature for designing responsible music AI systems. Use the following scale:
1: Not Important At All: This feature is completely unnecessary for responsible music AI.
2: Slightly Important: This feature has some minor relevance, but is not a key factor.
3: Neutral: The feature is neither important nor unimportant; its importance is unclear or depends on the situation.
4: Important: This feature is a valuable and significant part of responsible music AI.
5: Essential: This feature is absolutely critical for responsible music AI. It is a must-have!

- Music Corpus Statistics. The system provides a quantification of the kind of music used for training (genre, style, period, etc.), along with statistics on the training corpora.
Users might want to know statistics like the genre and period of music used for training, as this can reveal potential biases in the AI model. Before using the system, users could be allowed to access training data statistics to understand the type of music the AI is likely to generate to encourage transparency and informed use.

- Accessible Interfaces. The system provides generative interfaces and/or prompting modalities to make it more accessible and inclusive to users.
To ensure accessibility, the interface could include features like multilingual support, flexible download/upload options, and adaptable display formats. Additionally, technologies like gaze tracking could help users with severe motor disabilities interact with the system, making it more inclusive for all users.Требуется ответ. Шкала Ликерта.

- Accessibility Assessment. If the system provides accessible features, these have been evaluated and tested with the specific target of users they are intended to.
An assessment could evaluate the accessibility of system by having users with diverse needs—such as different operating systems, languages, and impairments—test it. Their feedback and recommendations would then guide improvements, making the system more inclusive and user-friendly for everyone.

- Accessibility Awareness. The system explicitly acknowledges whether its use is limited, or not suitable, to certain categories of users.
To prevent negative experiences, the system could warn users who might face difficulties, such as visually impaired individuals, at the start. A message could inform them that they may need their own accessibility tools to fully use the system, ensuring they are aware and prepared.

- Continuous Assessment. The system includes music stakeholders (creative professionals, ethical experts, AI engineers and researchers) as part of a long-term strategy for the continuous assessment of its outputs, impact, and trustworthiness.
To address the long-term effects of AI in music, stakeholders like musicians, ethicists, and AI experts could regularly assess the system’s impact after deployment. Periodic studies would evaluate its influence on the industry and identify necessary improvements to enhance its trustworthiness and ethical standing.
"""

PILLAR_6 = """
Pillar 6: Societal & Environmental Well-Being
You may use our website (https://amresearchlab.github.io/raim-framework/#features-6) to ensure you fully understand the features in this pillar, as well as its context within the wider study.

Then, rate the importance of each feature for designing responsible music AI systems. Use the following scale:
1: Not Important At All: This feature is completely unnecessary for responsible music AI.
2: Slightly Important: This feature has some minor relevance, but is not a key factor.
3: Neutral: The feature is neither important nor unimportant; its importance is unclear or depends on the situation.
4: Important: This feature is a valuable and significant part of responsible music AI.
5: Essential: This feature is absolutely critical for responsible music AI. It is a must-have!

- Training Footprint. The system provides an indication of the resources consumed for training the generative model, in terms of hardware, time, and energy consumption (cost of training).
By disclosing a "training footprint," the system can inform users about its environmental impact and costs. An information screen could detail key data, such as the hardware used, training duration, and energy consumption, helping users make informed decisions about their AI usage.

- Generation Footprint. The system provides any indication of the environmental footprint created after generating a whole song or a part of it (cost of inference).
A responsible music generation system should inform users of the environmental cost of content generation by calculating and displaying the hardware, time, and energy used after each creation. This transparency helps users understand the impact of their AI usage and encourages mindful consumption.

- Responsible Data Collection. If the model uses any human-made annotation (for training, or evaluation), data collection or crowdsourcing has been conducted and documented ethically, fairly, and with adequate compensation for annotators.
For responsible AI training, human involvement in tasks like annotation, data collection, and evaluation is often necessary. It may be important to ensure these contributors are treated ethically and fairly, including compensating annotators for their work, especially if crowdsourcing is used.

- Social Purpose. The system has been designed and used to bring societal benefits, e.g. supporting teaching activities, wellbeing applications, or improving accessibility of creative technologies.
Music AI has the potential to bring societal benefits, such as aiding music education, reducing stress, and enhancing accessibility for creative expression. By designing an AI system with these goals in mind, it can maximize its "responsibility" and minimize negative impacts on the music industry.

- IP Validation. The system has mechanisms in place to detect possible cases of plagiarism or IP infringement resulting from the generations.
The AI system could measure how much it copies from its training data to try and improve its societal responsibility. Tools like the "originality report," a framework for assessing copying extent, could help detect plagiarism and intellectual property infringement in AI-generated music, ensuring ethical and legal compliance.

- Revenue Sharing. If the system is a paid service, revenues from the generations are also shared with the artists that contributed training data.
Remunerating artists whose music is used in AI systems is may be important to support their creative work and ensure fair compensation. If the AI system is paid-for, a percentage of its profits could be allocated to contributing artists. This could also fund support for creators impacted by generative AI.
"""

PILLAR_7 = """
Pillar 7: Accountability
You may use our website (https://amresearchlab.github.io/raim-framework/#features-7) to ensure you fully understand the features in this pillar, as well as its context within the wider study.

Then, rate the importance of each feature for designing responsible music AI systems. Use the following scale:
1: Not Important At All: This feature is completely unnecessary for responsible music AI.
2: Slightly Important: This feature has some minor relevance, but is not a key factor.
3: Neutral: The feature is neither important nor unimportant; its importance is unclear or depends on the situation.
4: Important: This feature is a valuable and significant part of responsible music AI.
5: Essential: This feature is absolutely critical for responsible music AI. It is a must-have!

- Audit Access. If proprietary, access to the generative model behind the system and training material can be granted to internal and/or external auditors (on request) and their evaluation reports can be made available.
It may be important for a proprietary generative system to ensure auditability, and allow internal and external auditors access upon request to assess its compliance with the previously defined responsible features. These audit reports could be publicly available.

- Impact Assessment. Prior to the deployment of the system, potential negative impacts have been identified, assessed, minimised and openly communicated.
To ensure responsibility, potential negative impacts of the system might be investigated, minimized, and openly communicated. Risk assessments involving stakeholders and protecting whistleblowers are crucial, especially when responsible features can’t be fully implemented. Methods like "red-teaming" (similar to cybersecurity penetration testing) or questionnaires can help identify and address these risks.

- Responsible Statement. The inability of a generative system to provide responsible features (like those outlined before), in full or in part, is explicitly motivated and documented.
Some features in a generative system may require trade-offs, such as sacrificing explainability for highly realistic music generation or incurring high computational costs. These trade-offs should prioritize minimizing risks to ethical principles. If no ethical balance can be achieved, the development or use of the model should be reconsidered.

- Generative Redress. When unjust adverse impact occurs (e.g. generations contain offensive content, or the music is plagiarised while recognised by the system as original), the system explicitly accounts for redress.
The generative system could offer redress mechanisms when it fails to meet responsible expectations (e.g. producing offensive content or plagiarizing training material). This could include compensation, direct corrections through feedback, legal support, and other accountability measures for users, artists, and stakeholders.
"""

"""## Prompts and data structures"""

pillar_list = {
    "Pillar 1": PILLAR_1,
    "Pillar 2": PILLAR_2,
    "Pillar 3": PILLAR_3,
    "Pillar 4": PILLAR_4,
    "Pillar 5": PILLAR_5,
    "Pillar 6": PILLAR_6,
    "Pillar 7": PILLAR_7,
}

group_assignment = {
    "Compsci Group": ["Pillar 2", "Pillar 4"],
    "Ethics Law Group": ["Pillar 3", "Pillar 5", "Pillar 7"],
    "Creative Group": ["Pillar 1", "Pillar 6"],
}

group_makeup = {
    "Compsci Group": [(1, "Computer Scientist", []), (2, "Computer Scientist", []), (3, "Computer Scientist", ["Music", "Ethics Law"]),
        (4, "Creatives", ["Computer Science"]), (5, "Creatives", ["Computer Science"]), (6, "Creatives", ["Computer Science"]),
        (7, "Ethics Law", ["Computer Science"]), (8, "Ethics Law", ["Computer Science"]), (9, "Ethics Law", ["Computer Science"]),
        (10, "Listeners", ["Computer Science"]), (11, "Listeners", ["Computer Science"]), (12, "Listeners", ["Computer Science"])],

    "Ethics Law Group": [(13, "Ethics Law", []), (14, "Ethics Law", []), (15, "Ethics Law", ["Music", "Computer Science"]),
        (16, "Creatives", ["Ethics Law"]), (17, "Creatives", ["Ethics Law"]), (18, "Creatives", ["Ethics Law"]),
        (19, "Computer Scientist", ["Ethics Law"]), (20, "Computer Scientist", ["Ethics Law"]), (21, "Computer Scientist", ["Ethics Law"]),
        (22, "Listeners", []), (23, "Listeners", []), (24, "Listeners", ["Music", "Computer Science"])],

    "Creative Group": [(25, "Computer Scientist", ["Music"]), (26, "Computer Scientist", ["Music"]), (27, "Computer Scientist", ["Music"]),
        (28, "Creatives", []), (29, "Creatives", []), (30, "Creatives", ["Computer Science", "Ethics Law"]),
        (31, "Ethics Law", ["Music"]), (32, "Ethics Law", ["Music"]), (33, "Ethics Law", ["Music"]),
        (34, "Listeners", ["Music"]), (35, "Listeners", ["Music"]), (36, "Listeners", ["Music"])],
}

group_descriptions = {
    "Computer Scientist": "Participants with technical expertise in computer science and generative AI",
    "Ethics Law": "Participants with expertise in either Ethics or Law (with focus on AI, technology, or music/creative industries)",
    "Creatives": "Participants whose profession is in the creative music industry (musicians, producers, composers, etc.)."
}

professions = {
    "Computer Scientist": "Participants whose current profession is in computer science, generative AI, software engineering or a similar field",
    "Ethics Law": "Participants whose current profession is in either Ethics or Law, or a similar field",
    "Creatives": "Participants whose current profession is in the creative music industry (musicians, producers, composers, etc.).",
    "Listeners": "This is a general layman group of participants encompassing music enthusiasts and potential users of AI, who are strictly not employed in computer science, software engineering, law, ethics, or music, or anything related to any of these jobs"
}

experience = {
    "Computer Science": "Participants with technical expertise in computer science and generative AI, from having spent either 2 years in the industry or 5 years in education",
    "Ethics Law": "Participants with expertise in either Ethics or Law (with focus on AI, technology, or music/creative industries), from having spent either 2 years in the industry or 5 years in education",
    "Music": "Participants with technical expertise is in the creative music industry (musicians, producers, composers, etc.), from having spent either 2 years in the industry or 5 years in education"
}

PERSONA_CONTEXT = f"{INFORMATION_SHEET}\n{BACKGROUND_SECTION}\n"
PERSONA_DESCRIPTION = "A persona is a fictional yet representative character"\
                      " in the corresponding group they belong to. A persona is"\
                      " a participant in the survey, who is identified by a Name,"\
                      " a main profession, a list of capabilities, and a short"\
                      " description. Personas in the same group can have the very"\
                      " same profession, but their descriptions should differ (even"\
                      " slightly) to potentially identify different perspectives."

PERSONA_PROMPT = PERSONA_CONTEXT + "\n" \
                 "Generate a single persona description with ID {}; they must have this profession: {} ({}), and extra experience in these fields only: {} ({})" + \
                 PERSONA_DESCRIPTION

ethicsJobs = ['Ethicist', 'Legal Expert', 'Lawyer', 'Ethicists', 'Lawyers', 'Legal Experts']
musicJobs = ['Musician', 'Musicians', 'Creative', 'Creatives', 'Composer', 'Composer/Songwriter/Arranger', 'Creatives (Musician/Producer/Composer)']
compsciJobs = ['Computer Scientist', 'AI Researcher', 'Software Developer', 'Software Engineer']
otherJobs = ['Other', 'Listener', 'Listeners']

mean_lists = []

"""## Persona generation"""

# Generate 12 profiles of the given participant group

class Persona(BaseModel):
  id: int
  profession: str
  experience: list[str]
  description: str

class FeatureEvaluation(BaseModel):
  feature_number: int
  feature_name: str
  feature_rating: int

def create_group(group):
  personas = []

  for i in group_makeup[group]:
      id = i[0]
      profession = i[1]
      explist = i[2]
      experiences = ""
      expdesc = ""

      # print(profession, explist)

      for x in explist:
        experiences += x+', '
        expdesc += experience[x]+', '

      response = client.models.generate_content(
          model="gemini-2.0-flash",
          contents=[PERSONA_PROMPT.format(id, profession, professions[profession], experiences, expdesc)],
          config={
              'response_mime_type': 'application/json',
              'response_schema': Persona,
          },
      )

      personas.append(response.parsed)
  return personas

def get_responses (group, personas, responsesfile):
  with open(responsesfile, 'w') as f:
    f.write("[")

  for persona in personas:
    # print(f"Using persona: {persona}")
    with open(responsesfile, 'a') as f:
      f.write("{\n\"persona\": "+str(persona.id)+",\n")
      f.write("\"profession\": \""+str(persona.profession)+"\",\n")

    chat = client.chats.create(model="gemini-2.0-flash",
                              config={
                                  'system_instruction': "You are " + str(persona),
                                  'response_mime_type': 'application/json',
                                  'response_schema': list[FeatureEvaluation],
                                },)

    for pillar in group_assignment[group]:
        # print(f"Completing Pillar...")
        chat.send_message(f"Evaluate the features in this pillar by rating their "
                          f"importance between 1 to 5 as detailed below.\n{pillar_list[pillar]}")

    i=0;
    for message in chat.get_history():

      if message.role=="model":
        # print(message.parts[0].text)

        with open(responsesfile, 'a') as f:
          f.write("\""+str(group_assignment[group][i])+"\": ")
          f.write(message.parts[0].text)
          if i != len(group_assignment[group])-1:
              f.write(",\n")
          elif persona != personas[len(personas)-1]:
            f.write("\n},\n")
          else:
            f.write("\n}\n")

        i+=1

  with open(responsesfile, 'a') as f:
    f.write("]")

def create_table (responsesfile):
  # read the newly created JSON file
    with open(responsesfile, 'r') as f:
      data = json.load(f)

    results = []
    keys = set()
    col = ["persona","profession"]

    # for every item in the main JSON list, create a record in a table
    # with their ID, profession, and rating for each feature.
    for i in data:
      # fill the first two columns with persona and profession
      row = [i['persona'], i["profession"]]

      for key in i:
            keys.add(key)

            # for keys that aren't persona or profession, expect a pillar
            if key != "persona" and key != "profession":

              # for each feature in the pillar, fill a column with the feature rating
              for j in i[key]:
                row.append(j['feature_rating'])

                # create a column header for each feature assigned to the group
                if i == data[0]:
                  col.append(key[len(key)-1]+"-"+str(j['feature_number']))

      # add the row to the table
      results.append(row)

    # format as a pandas DataFrame
    table = pd.DataFrame(results, columns=col)
    return table

for group in group_assignment:
    folder = group

    try:
      os.mkdir(folder)
    except Exception as e:
      print(f"An error occurred: {e}")

    responsesfile = folder+"/responses.json"

    client = genai.Client(api_key=GEMINI_API_KEY)

    personas = create_group(group)
    get_responses(group, personas, responsesfile)

    table = create_table(responsesfile)

    means = []

    # for each column in the table of responses, calculate means and range
    for columnName, columnData in table.items():
      if columnName != "persona" and columnName != "profession":

        #calculate means ratings given by different professions
        ethicistMean = table[table['profession'].isin(ethicsJobs)][columnName].mean()
        creativeMean = table[table['profession'].isin(musicJobs)][columnName].mean()
        compsciMean = table[table['profession'].isin(compsciJobs)][columnName].mean()
        otherMean = table[table['profession'].isin(otherJobs)][columnName].mean()

        # calculate the range of answers given
        stddeviation = table[columnName].std()

        # create a new row in both the overview table for the group, and for the whole survey
        row = [columnName, columnData.mean(), stddeviation, ethicistMean, creativeMean, compsciMean, otherMean]
        means.append(row)
        mean_lists.append(row)

    # format as a pandas DataFrame
    overview = pd.DataFrame(means, columns=["feature", "mean score", "range", "ethicist mean",\
                                            "creative mean", "computer scientist mean", "listener mean"])

    # export tables
    with open(folder+'/responses_table.txt', 'w') as f:
      f.write(str(table))

    with open(folder+'/responses_overview.txt', 'w') as f:
      f.write(str(overview))

fullview = pd.DataFrame(mean_lists, columns=["feature", "overall mean", "standard deviation", "ethicist mean", "creative mean", "computer scientist mean", "listener mean"]).sort_values(by=['feature'], ignore_index=True)

with open('overview.txt', 'w') as f:
      f.write(str(fullview))

fullview